from random import random
import numpy as np
import pandas as pd
import torch
import torch.utils.data
import torch.optim as optim
from torch.optim import Adam
from torch.nn import functional as F
from torch.nn import (Dropout, LeakyReLU, Linear, Module, ReLU, Sequential,
Conv2d, ConvTranspose2d, BatchNorm2d, Sigmoid, init, BCELoss, CrossEntropyLoss,SmoothL1Loss)
from model.synthesizer.stacked_condvec_factory import StackedCondvecFactory
from model.synthesizer.transformer import ImageTransformer,DataTransformer
from tqdm import tqdm
from model.synthesizer.ctabgan_synthesizer import CTABGANSynthesizer, Sampler, Generator, Discriminator, Classifier, apply_activate, determine_layers_disc, determine_layers_gen, get_st_ed, weights_init, cond_loss


class CTABGANFirstLayer(CTABGANSynthesizer):

    """
    This class represents the main model used for training the model and generating synthetic data


    Variables:
    1) random_dim -> size of the noise vector fed to the generator
    2) class_dim -> tuple containing dimensionality of hidden layers for the classifier network
    3) num_channels -> no. of channels for deciding respective hidden layers of discriminator and generator networks
    4) dside -> height/width of the input data fed to discriminator network
    5) gside -> height/width of the input data generated by the generator network
    6) l2scale -> parameter to decide strength of regularization of the network based on constraining l2 norm of weights
    7) batch_size -> no. of records to be processed in each mini-batch of training
    8) epochs -> no. of epochs to train the model
    9) device -> type of device to be used for training (i.e., gpu/cpu)
    10) generator -> generator network from which data can be generated after training the model

    Methods:
    1) __init__() -> initializes the model with user specified parameters
    2) fit() -> takes the pre-processed training data and associated parameters as input to fit the CTABGANSynthesizer model 
    3) sample() -> takes as input the no. of data rows to be generated and synthesizes the corresponding no. of data rows

    """ 
    
    def __init__(self,
                 class_dim=(256, 256, 256, 256),
                 random_dim=100,
                 num_channels=64,
                 l2scale=1e-5,
                 batch_size=500,
                 epochs=1):
        super().__init__(
            class_dim = class_dim,
            random_dim = random_dim,
            num_channels = num_channels,
            l2scale=l2scale,
            batch_size=batch_size,
            epochs=epochs
        ) 
        
    def sample_train(self, n, stacked_condvec_factory):
        
        # turning the generator into inference mode to effectively use running statistics in batch norm layers
        self.generator.eval()
        # column information associated with the transformer fit to the pre-processed training data
        output_info = self.transformer.output_info
        
        # generating synthetic data in batches accordingly to the total no. required
        steps = n // self.batch_size + 1
        data = []        
        
        print("Stacked condvec factory n_opt first layer: " + str(stacked_condvec_factory.n_opt))
        
        for _ in range(steps):
            # generating synthetic data using sampled noise and conditional vectors
            noisez = torch.randn(self.batch_size, self.random_dim, device=self.device)
            condvec = stacked_condvec_factory.sample_train()
            c, m, col, opt = condvec
            c = torch.from_numpy(c).to(self.device)
            noisez = torch.cat([noisez, c], dim=1)
            noisez =  noisez.view(self.batch_size,self.random_dim + stacked_condvec_factory.n_opt,1,1)
            fake = self.generator(noisez)
            faket = self.Gtransformer.inverse_transform(fake)
            fakeact = apply_activate(faket,output_info)

            data.append(fakeact.detach().cpu().numpy())

        data = np.concatenate(data, axis=0)

        return data

    def sample(self, n, stacked_condvec_factory):
        # turning the generator into inference mode to effectively use running statistics in batch norm layers
        self.generator.eval()
        # column information associated with the transformer fit to the pre-processed training data
        output_info = self.transformer.output_info
        
        # generating synthetic data in batches accordingly to the total no. required
        steps = n // self.batch_size + 1
        data = []        
        
        print("Stacked condvec factory n_opt first layer: " + str(stacked_condvec_factory.n_opt))
        
        for i in range(steps):
            # generating synthetic data using sampled noise and conditional vectors
            noisez = torch.randn(self.batch_size, self.random_dim, device=self.device)
            condvec = stacked_condvec_factory.sample()
            c = condvec
            c = torch.from_numpy(c).to(self.device)
            noisez = torch.cat([noisez, c], dim=1)
            noisez =  noisez.view(self.batch_size,self.random_dim + stacked_condvec_factory.n_opt,1,1)
            fake = self.generator(noisez)
            faket = self.Gtransformer.inverse_transform(fake)
            fakeact = apply_activate(faket,output_info)

            data.append(fakeact.detach().cpu().numpy())

        data = np.concatenate(data, axis=0)

        return data
        